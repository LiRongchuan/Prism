import time
import heapq
import logging
import threading
from typing import Dict, List, Set
from collections import defaultdict
from sglang.srt.managers.io_struct import GenerateReqInput

logger = logging.getLogger(__name__)

PREFILL_RATE = 1.0 / 2048
DEFAULT_PREFILL_LEN = 1024
LOWER_PREFILL_BOUND = 0.05
UPPER_PREFILL_BOUND = 20
MAX_QUEUE_LEN = 10
GPU_SIZE = 80 * (1 << 30)

class RequestWrapper:
    """å°è£…è¯·æ±‚å¯¹è±¡ï¼Œèµ‹å€¼ä¼˜å…ˆçº§"""
    def __init__(self, req: GenerateReqInput):
        self.req = req
        self.model_name = req.model
        self.priority = self._calculate_priority(req)  # Lower value means higher priority (min-heap)

    def _calculate_priority(self, req: GenerateReqInput):
        """è®¡ç®—ä¼˜å…ˆçº§
        priority = arrival_time + slo - prefill_time
        åˆ°è¾¾è¶Šæ—©ï¼Œsloè¶Šç´§å¼ ï¼Œä¼˜å…ˆçº§è¶Šä½
        """
        def clamp(x, lower, upper): # ç¡®ä¿æ—¶é—´å¤„äºåŒºé—´èŒƒå›´å†…
            return max(lower, min(x, upper))
        profiled_prefill_time = (
            req.prompt_len * PREFILL_RATE
            if req.prompt_len is not None
            else DEFAULT_PREFILL_LEN * PREFILL_RATE
        )
        profiled_prefill_time = clamp(profiled_prefill_time, LOWER_PREFILL_BOUND, UPPER_PREFILL_BOUND)
        return req.arrival_time + req.slo - profiled_prefill_time

    def __lt__(self, other):
        return self.priority < other.priority  # ç”¨äºæ’åº

    def __str__(self):
        return f"RequestWrapper(model_name={self.model_name}, priority={self.priority}, req_id={self.req.rid})"

    def __repr__(self):
        return self.__str__()


class RequestQueue:
    """ç»´æŠ¤å¤šæ¨¡å‹è¯·æ±‚ä¼˜å…ˆé˜Ÿåˆ—
    èµ„æºè·Ÿè¸ªï¼Œè¯·æ±‚å‡†å…¥
    å¯¹æ¨¡å‹rank0 GPUè·Ÿè¸ª
    """
    def __init__(self, model_name_to_cell_size: Dict[str, int]):
        self._model_name_to_cell_size = model_name_to_cell_size
        self._queue: List[RequestWrapper] = []  # å°é¡¶ä¼˜å…ˆé˜Ÿåˆ—ï¼Œç»´æŠ¤GPUè¯·æ±‚
        self._model_requests: Dict[str, Set[RequestWrapper]] = defaultdict(set) # åˆ†æ¨¡å‹ç»´æŠ¤è¯·æ±‚é˜Ÿåˆ—
        self._lock = threading.Lock() # é˜Ÿåˆ—é”
        self.last_log_time = 0
        # è¿è¡Œä¸­æ¨¡å‹æ˜¾å­˜å ç”¨å ç”¨ï¼Œæœªä½“ç°åœ¨ç‰©ç†æ˜¾å­˜å˜åŒ–ä¸­ï¼Œéœ€è¦è‡ªå·±è·Ÿè¸ª
        self._activating_usage_by_model = defaultdict(float)

    def empty(self) -> bool:
        """æ¸…ç©ºé˜Ÿåˆ—"""
        with self._lock:
            return len(self._queue) == 0

    def pop_model_requests(self, model_name: str) -> List[GenerateReqInput]:
        """å¼¹å‡ºæŒ‡å®šæ¨¡å‹æ‰€æœ‰è¯·æ±‚"""
        if model_name not in self._model_requests: return []
        with self._lock:
            model_reqs = list(self._model_requests[model_name])
            self._queue = [req for req in self._queue if req not in model_reqs]
            heapq.heapify(self._queue) # é‡æ–°æ’åº
            del self._model_requests[model_name]
            return [req_wrapper.req for req_wrapper in model_reqs] # è¿”å›æ‰€æœ‰åŸè¯·æ±‚å¯¹è±¡

    def add_requests(self, reqs: List[GenerateReqInput]):
        """æ·»åŠ è¯·æ±‚batch"""
        wrapped_reqs = [RequestWrapper(req) for req in reqs]
        with self._lock:
            for wrapped_req in wrapped_reqs:
                heapq.heappush(self._queue, wrapped_req)
                self._model_requests[wrapped_req.model_name].add(wrapped_req)

    def remove_model_requests(self, model_name):
        """æ¸…é™¤ä½†ä¸è¿”å›åˆ¶å®šæ¨¡å‹æ‰€æœ‰è¯·æ±‚"""
        if model_name not in self._model_requests: return
        with self._lock:
            removed = self._model_requests[model_name]
            self._queue = [req for req in self._queue if req not in removed]
            heapq.heapify(self._queue)
            del self._model_requests[model_name]
    
    def log_info(self, info: str):
        """é™åˆ¶logé€Ÿç‡ï¼Œæ¯ç§’è‡³å¤šä¸€æ¡"""
        current_time = time.time()
        if current_time - self.last_log_time > 1:
            logger.info(info)
            self.last_log_time = current_time

    def admission_control(
        self,
        model_states: Dict[str, str],
        available_resources: float, # å‰©ä½™KV cache
        model_backend_queue_lens: Dict[str, int], # Engineä¾§é˜Ÿåˆ—é•¿åº¦
        allow_sending_when_activating: bool = False,
    ) -> Dict[str, List[GenerateReqInput]]:
        """å‡†å…¥æ§åˆ¶
        Memory Track
        |â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|------------------|               |
        | activated usage | activating usage | net available |
        |  tracked usage  |        available resource        |
        """
        admitted = defaultdict(list)
        total_activating_usage = sum(self._activating_usage_by_model.values()) # æ¿€æ´»ä¸­æ¨¡å‹æ˜¾å­˜å ç”¨
        # Note: Using infinity here as the actual implementation doesn't seem to limit resources
        net_available = float("inf")
        # net_available = available_resources - total_activating_usage
        if net_available <= 0:
            self.log_info(f"ğŸ˜Ÿ Resource ran out, net_available: {net_available}, queue_len: {len(self._queue)}")
            self.log_info(f"Activating usages: {self._activating_usage_by_model}")
            return admitted
        if len(self._queue) == 0:
            self.log_info(f"ğŸ˜ƒ No request queuing")
            return admitted
        # åç«¯é˜Ÿåˆ—å¤ªé•¿æ—¶è·³è¿‡
        models_to_skip = {
            model_name for model_name, queue_len 
            in model_backend_queue_lens.items()
            if queue_len > MAX_QUEUE_LEN
        }
        put_backs = []
        with self._lock:
            while self._queue and net_available > 0:
                # æ˜¾å­˜å……è¶³æ—¶é€ä¸ªæ·»åŠ è¯·æ±‚ï¼Œä¸å¯ç”¨è¯·æ±‚æ”¾å›ç­‰å¾…é˜Ÿåˆ—
                req_wrapper = heapq.heappop(self._queue)
                model_name = req_wrapper.model_name
                model_state = model_states.get(model_name, "deactivated")
                if model_name in models_to_skip:
                    put_backs.append(req_wrapper) # æ’é˜Ÿå¤ªé•¿
                    self.log_info(f"â° Queuing exceeds limit, model queue: {model_backend_queue_lens[model_name]}")
                    continue
                if model_state in ("deactivating", "deactivated"):
                    put_backs.append(req_wrapper) # æ¨¡å‹æœªæ¿€æ´»
                    self.log_info(f"ğŸ’¤ {model_name} deactivated")
                    continue
                if model_state == "activating" and not allow_sending_when_activating:
                    put_backs.append(req_wrapper) # æ¨¡å‹ä¸æ¥å—è¯·æ±‚
                    self.log_info(f"ğŸ”• {model_name} does not allow message while activating")
                    continue
                resources_needed = self._get_request_resources(req_wrapper.req)
                if net_available >= resources_needed:
                    net_available -= resources_needed
                    admitted[model_name].append(req_wrapper.req) # æ·»åŠ å……è¶³è¯·æ±‚
                    self._model_requests[model_name].remove(req_wrapper)
                    if not self._model_requests[model_name]: del self._model_requests[model_name]
                    if model_state == "activating": self._activating_usage_by_model[model_name] += resources_needed
                else:
                    put_backs.append(req_wrapper) # èµ„æºä¸è¶³
                    self.log_info(f"ğŸ˜¢ Resource limited, net_available: {net_available}, queue_len: {len(self._queue)}")
                    break
            # ç»´æŠ¤ç­‰å¾…é˜Ÿåˆ—
            put_backs.extend(self._queue)
            self._queue = put_backs
            heapq.heapify(self._queue)
            self.log_info(
                f"ğŸ“° Resource update: net_available: {net_available}, queue_len: {len(self._queue)}, model_backend_queue_lens: {model_backend_queue_lens}"
            )
        return admitted

    def _get_request_resources(self, req: GenerateReqInput) -> float:
        """ä¼°è®¡è¯·æ±‚æ˜¾å­˜å ç”¨"""
        cell_size = self._model_name_to_cell_size[req.model]
        input_len = (
            req.prompt_len
            if req.prompt_len is not None and req.prompt_len > 0
            else DEFAULT_PREFILL_LEN
        )
        return cell_size * (input_len + 20)

    def clear_activating_usage(self, model_name: str):
        """æ¸…ç©ºæ¿€æ´»ä¸­æ¨¡å‹èµ„æºè·Ÿè¸ª"""
        with self._lock:
            if model_name in self._activating_usage_by_model:
                del self._activating_usage_by_model[model_name]

    def __len__(self) -> int:
        return len(self._queue)

    def __repr__(self) -> str:
        if len(self._model_requests) == 0:
            req_counts_str = ""
        else:
            req_counts_str = ", ".join([
                f"{model_name}: {len(reqs)}"
                for model_name, reqs in self._model_requests.items()
            ])
        return f"RequestQueue(total_queued={len(self._queue)}, {req_counts_str})"
